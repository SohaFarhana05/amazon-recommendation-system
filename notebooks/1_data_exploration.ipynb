{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436ee5b0",
   "metadata": {},
   "source": [
    "# Amazon Product Recommendation System Using Multi-Modal Learning\n",
    "\n",
    "## Data Science Project for Amazon Internship\n",
    "\n",
    "**Author:** Soha Farhana  \n",
    "**Date:** July 14, 2025  \n",
    "**Project:** Multi-modal recommendation system combining text, image, and behavioral data\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "This notebook implements a comprehensive recommendation system that leverages:\n",
    "- **Product metadata** (text descriptions, categories)\n",
    "- **Customer behavior data** (browsing history, purchase patterns)\n",
    "- **Visual features** (product images)\n",
    "- **Temporal patterns** (seasonal trends, recent interests)\n",
    "\n",
    "### Business Value\n",
    "- Improves conversion rates through better recommendations\n",
    "- Addresses cold-start problems for new products/customers\n",
    "- Provides explainable AI with feature importance\n",
    "- Enables A/B testing against existing systems\n",
    "\n",
    "### Technical Approach\n",
    "- **Multi-modal fusion architecture** combining collaborative and content-based filtering\n",
    "- **BERT embeddings** for text processing\n",
    "- **CNN features** for image analysis\n",
    "- **Temporal attention** for sequential patterns\n",
    "- **Production-ready API** with Docker deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad40f8",
   "metadata": {},
   "source": [
    "## 1. Project Setup and Data Loading\n",
    "\n",
    "Setting up the development environment and loading Amazon product dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run this if packages are not installed)\n",
    "# !pip install torch torchvision transformers scikit-learn pandas numpy matplotlib seaborn plotly\n",
    "# !pip install sentence-transformers faiss-cpu tqdm pyyaml mlflow flask\n",
    "\n",
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# NLP and Computer Vision\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import logging\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive sample Amazon dataset\n",
    "def create_sample_amazon_data():\n",
    "    \"\"\"Create realistic sample Amazon product and review data\"\"\"\n",
    "    \n",
    "    # Sample product categories\n",
    "    categories = ['Electronics', 'Books', 'Home_and_Kitchen', 'Clothing', 'Sports', 'Toys', 'Beauty']\n",
    "    brands = ['Amazon', 'Apple', 'Samsung', 'Sony', 'Nike', 'Adidas', 'Generic']\n",
    "    \n",
    "    # Generate product metadata\n",
    "    np.random.seed(42)\n",
    "    n_products = 1000\n",
    "    \n",
    "    products_data = []\n",
    "    for i in range(n_products):\n",
    "        category = np.random.choice(categories)\n",
    "        brand = np.random.choice(brands)\n",
    "        \n",
    "        product = {\n",
    "            'asin': f'B{str(i+1).zfill(6)}',\n",
    "            'title': f'{brand} {category} Product {i+1}',\n",
    "            'description': f'High-quality {category.lower()} product from {brand}. '\n",
    "                          f'Features advanced technology and great design. '\n",
    "                          f'Perfect for daily use and professional applications.',\n",
    "            'price': np.round(np.random.lognormal(3, 1), 2),\n",
    "            'brand': brand,\n",
    "            'category': category,\n",
    "            'image_url': f'https://example.com/images/{i+1}.jpg',\n",
    "            'average_rating': np.round(np.random.normal(4.0, 0.8), 1),\n",
    "            'review_count': np.random.poisson(50),\n",
    "            'availability': np.random.choice(['In Stock', 'Limited', 'Out of Stock'], p=[0.8, 0.15, 0.05])\n",
    "        }\n",
    "        products_data.append(product)\n",
    "    \n",
    "    # Generate user data\n",
    "    n_users = 5000\n",
    "    users_data = []\n",
    "    \n",
    "    for i in range(n_users):\n",
    "        user = {\n",
    "            'user_id': f'U{str(i+1).zfill(6)}',\n",
    "            'age_group': np.random.choice(['18-25', '26-35', '36-45', '46-55', '55+']),\n",
    "            'location': np.random.choice(['US', 'UK', 'Canada', 'Germany', 'Japan']),\n",
    "            'prime_member': np.random.choice([True, False], p=[0.6, 0.4]),\n",
    "            'signup_date': datetime.now() - timedelta(days=np.random.randint(30, 1095))\n",
    "        }\n",
    "        users_data.append(user)\n",
    "    \n",
    "    # Generate review/interaction data\n",
    "    n_reviews = 15000\n",
    "    reviews_data = []\n",
    "    \n",
    "    for i in range(n_reviews):\n",
    "        user_idx = np.random.randint(0, n_users)\n",
    "        product_idx = np.random.randint(0, n_products)\n",
    "        \n",
    "        # Simulate some preference patterns\n",
    "        user_location = users_data[user_idx]['location']\n",
    "        product_category = products_data[product_idx]['category']\n",
    "        \n",
    "        # Location-based preferences\n",
    "        rating_bias = 0\n",
    "        if user_location == 'US' and product_category in ['Electronics', 'Books']:\n",
    "            rating_bias = 0.3\n",
    "        elif user_location == 'Japan' and product_category == 'Electronics':\n",
    "            rating_bias = 0.5\n",
    "        \n",
    "        base_rating = products_data[product_idx]['average_rating']\n",
    "        rating = np.clip(np.random.normal(base_rating + rating_bias, 0.8), 1, 5)\n",
    "        \n",
    "        review = {\n",
    "            'user_id': users_data[user_idx]['user_id'],\n",
    "            'asin': products_data[product_idx]['asin'],\n",
    "            'rating': np.round(rating, 1),\n",
    "            'review_text': f'This {product_category.lower()} product is {\"excellent\" if rating >= 4 else \"okay\"}. '\n",
    "                          f'{\"Highly recommend!\" if rating >= 4.5 else \"Worth considering.\"}',\n",
    "            'helpful_votes': np.random.poisson(2),\n",
    "            'verified_purchase': np.random.choice([True, False], p=[0.8, 0.2]),\n",
    "            'review_date': datetime.now() - timedelta(days=np.random.randint(1, 365))\n",
    "        }\n",
    "        reviews_data.append(review)\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    products_df = pd.DataFrame(products_data)\n",
    "    users_df = pd.DataFrame(users_data)\n",
    "    reviews_df = pd.DataFrame(reviews_data)\n",
    "    \n",
    "    # Create data directories\n",
    "    os.makedirs('../data/raw', exist_ok=True)\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    os.makedirs('../data/sample', exist_ok=True)\n",
    "    \n",
    "    # Save datasets\n",
    "    products_df.to_csv('../data/sample/products.csv', index=False)\n",
    "    users_df.to_csv('../data/sample/users.csv', index=False)\n",
    "    reviews_df.to_csv('../data/sample/reviews.csv', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Created sample dataset with:\")\n",
    "    print(f\"   üì¶ {len(products_df)} products\")\n",
    "    print(f\"   üë• {len(users_df)} users\")\n",
    "    print(f\"   ‚≠ê {len(reviews_df)} reviews\")\n",
    "    \n",
    "    return products_df, users_df, reviews_df\n",
    "\n",
    "# Create sample data\n",
    "products_df, users_df, reviews_df = create_sample_amazon_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424bea5",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Cleaning and preprocessing data for collaborative filtering and content-based recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration and visualization\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"Products shape: {products_df.shape}\")\n",
    "print(f\"Users shape: {users_df.shape}\")\n",
    "print(f\"Reviews shape: {reviews_df.shape}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nüìà Basic Statistics:\")\n",
    "print(\"\\nProducts DataFrame:\")\n",
    "print(products_df.info())\n",
    "print(products_df.describe())\n",
    "\n",
    "print(\"\\nReviews DataFrame:\")\n",
    "print(reviews_df.info())\n",
    "print(reviews_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nüîç Missing Values:\")\n",
    "print(\"Products missing values:\", products_df.isnull().sum().sum())\n",
    "print(\"Users missing values:\", users_df.isnull().sum().sum())\n",
    "print(\"Reviews missing values:\", reviews_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=3,\n",
    "    subplot_titles=('Rating Distribution', 'Category Distribution', 'Price Distribution',\n",
    "                   'Reviews per User', 'Reviews per Product', 'Temporal Patterns'),\n",
    "    specs=[[{\"type\": \"histogram\"}, {\"type\": \"bar\"}, {\"type\": \"histogram\"}],\n",
    "           [{\"type\": \"histogram\"}, {\"type\": \"histogram\"}, {\"type\": \"scatter\"}]]\n",
    ")\n",
    "\n",
    "# Rating distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=reviews_df['rating'], name='Ratings', nbinsx=10),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Category distribution\n",
    "category_counts = products_df['category'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=category_counts.index, y=category_counts.values, name='Categories'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Price distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=products_df['price'], name='Prices', nbinsx=30),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "# Reviews per user\n",
    "user_review_counts = reviews_df['user_id'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=user_review_counts.values, name='Reviews per User', nbinsx=20),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Reviews per product\n",
    "product_review_counts = reviews_df['asin'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=product_review_counts.values, name='Reviews per Product', nbinsx=20),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Temporal patterns\n",
    "reviews_df['review_month'] = pd.to_datetime(reviews_df['review_date']).dt.to_period('M')\n",
    "temporal_counts = reviews_df['review_month'].value_counts().sort_index()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=temporal_counts.index.astype(str), y=temporal_counts.values, \n",
    "               mode='lines+markers', name='Reviews over Time'),\n",
    "    row=2, col=3\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800, \n",
    "    title_text=\"Amazon Dataset Comprehensive Analysis\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "print(f\"Average rating: {reviews_df['rating'].mean():.2f}\")\n",
    "print(f\"Rating standard deviation: {reviews_df['rating'].std():.2f}\")\n",
    "print(f\"Most popular category: {category_counts.index[0]} ({category_counts.iloc[0]} products)\")\n",
    "print(f\"Average price: ${products_df['price'].mean():.2f}\")\n",
    "print(f\"Price range: ${products_df['price'].min():.2f} - ${products_df['price'].max():.2f}\")\n",
    "print(f\"Average reviews per user: {user_review_counts.mean():.1f}\")\n",
    "print(f\"Average reviews per product: {product_review_counts.mean():.1f}\")\n",
    "\n",
    "# Sparsity analysis\n",
    "total_possible_interactions = len(users_df) * len(products_df)\n",
    "actual_interactions = len(reviews_df)\n",
    "sparsity = (1 - actual_interactions / total_possible_interactions) * 100\n",
    "print(f\"\\nüîç Data Sparsity: {sparsity:.2f}% (typical for recommendation systems)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced342a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing for collaborative filtering\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, min_user_interactions=3, min_item_interactions=3):\n",
    "        self.min_user_interactions = min_user_interactions\n",
    "        self.min_item_interactions = min_item_interactions\n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.item_encoder = LabelEncoder()\n",
    "        \n",
    "    def preprocess_interactions(self, reviews_df):\n",
    "        \"\"\"Filter and encode user-item interactions\"\"\"\n",
    "        # Filter users and items with minimum interactions\n",
    "        user_counts = reviews_df['user_id'].value_counts()\n",
    "        item_counts = reviews_df['asin'].value_counts()\n",
    "        \n",
    "        valid_users = user_counts[user_counts >= self.min_user_interactions].index\n",
    "        valid_items = item_counts[item_counts >= self.min_item_interactions].index\n",
    "        \n",
    "        filtered_reviews = reviews_df[\n",
    "            (reviews_df['user_id'].isin(valid_users)) & \n",
    "            (reviews_df['asin'].isin(valid_items))\n",
    "        ].copy()\n",
    "        \n",
    "        # Encode users and items\n",
    "        filtered_reviews['user_idx'] = self.user_encoder.fit_transform(filtered_reviews['user_id'])\n",
    "        filtered_reviews['item_idx'] = self.item_encoder.fit_transform(filtered_reviews['asin'])\n",
    "        \n",
    "        print(f\"‚úÖ Filtered data: {len(filtered_reviews)} interactions\")\n",
    "        print(f\"   Users: {len(valid_users)} ‚Üí {filtered_reviews['user_idx'].nunique()}\")\n",
    "        print(f\"   Items: {len(valid_items)} ‚Üí {filtered_reviews['item_idx'].nunique()}\")\n",
    "        \n",
    "        return filtered_reviews\n",
    "    \n",
    "    def create_interaction_matrix(self, filtered_reviews):\n",
    "        \"\"\"Create user-item interaction matrix\"\"\"\n",
    "        n_users = filtered_reviews['user_idx'].nunique()\n",
    "        n_items = filtered_reviews['item_idx'].nunique()\n",
    "        \n",
    "        # Create interaction matrix\n",
    "        interaction_matrix = np.zeros((n_users, n_items))\n",
    "        \n",
    "        for _, row in filtered_reviews.iterrows():\n",
    "            user_idx = row['user_idx']\n",
    "            item_idx = row['item_idx']\n",
    "            rating = row['rating']\n",
    "            interaction_matrix[user_idx, item_idx] = rating\n",
    "        \n",
    "        print(f\"üìä Interaction matrix shape: {interaction_matrix.shape}\")\n",
    "        print(f\"   Sparsity: {(np.count_nonzero(interaction_matrix) / interaction_matrix.size * 100):.2f}% filled\")\n",
    "        \n",
    "        return interaction_matrix\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessor = DataPreprocessor()\n",
    "filtered_reviews = preprocessor.preprocess_interactions(reviews_df)\n",
    "interaction_matrix = preprocessor.create_interaction_matrix(filtered_reviews)\n",
    "\n",
    "# Train-test split for recommendation evaluation\n",
    "def create_train_test_split(filtered_reviews, test_ratio=0.2):\n",
    "    \"\"\"Create temporal train-test split\"\"\"\n",
    "    # Sort by review date\n",
    "    sorted_reviews = filtered_reviews.sort_values('review_date')\n",
    "    \n",
    "    # Split per user to ensure all users in both sets\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for user_idx in sorted_reviews['user_idx'].unique():\n",
    "        user_reviews = sorted_reviews[sorted_reviews['user_idx'] == user_idx]\n",
    "        \n",
    "        if len(user_reviews) >= 2:\n",
    "            n_test = max(1, int(len(user_reviews) * test_ratio))\n",
    "            test_reviews = user_reviews.tail(n_test)\n",
    "            train_reviews = user_reviews.head(len(user_reviews) - n_test)\n",
    "            \n",
    "            train_data.append(train_reviews)\n",
    "            test_data.append(test_reviews)\n",
    "        else:\n",
    "            train_data.append(user_reviews)\n",
    "    \n",
    "    train_df = pd.concat(train_data, ignore_index=True)\n",
    "    test_df = pd.concat(test_data, ignore_index=True) if test_data else pd.DataFrame()\n",
    "    \n",
    "    print(f\"üìä Train-Test Split:\")\n",
    "    print(f\"   Train: {len(train_df)} interactions\")\n",
    "    print(f\"   Test: {len(test_df)} interactions\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = create_train_test_split(filtered_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad2055",
   "metadata": {},
   "source": [
    "## 3. Text Feature Extraction with BERT\n",
    "\n",
    "Implementing BERT-based embeddings for product descriptions and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT-based text feature extraction\n",
    "class TextFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=128):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def prepare_text_data(self, products_df):\n",
    "        \"\"\"Combine and clean text features\"\"\"\n",
    "        # Combine title, description, and category\n",
    "        products_df['combined_text'] = (\n",
    "            products_df['title'].fillna('') + ' ' + \n",
    "            products_df['description'].fillna('') + ' ' + \n",
    "            products_df['category'].fillna('')\n",
    "        )\n",
    "        \n",
    "        # Clean text\n",
    "        products_df['combined_text'] = products_df['combined_text'].str.lower()\n",
    "        products_df['combined_text'] = products_df['combined_text'].str.replace('[^a-zA-Z0-9 ]', '', regex=True)\n",
    "        \n",
    "        return products_df['combined_text'].tolist()\n",
    "    \n",
    "    def extract_embeddings(self, texts, batch_size=32):\n",
    "        \"\"\"Extract BERT embeddings for text data\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        print(f\"ü§ñ Extracting BERT embeddings for {len(texts)} products...\")\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            encoded = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = encoded['input_ids'].to(self.device)\n",
    "            attention_mask = encoded['attention_mask'].to(self.device)\n",
    "            \n",
    "            # Extract embeddings\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                # Use [CLS] token embedding\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                embeddings.append(batch_embeddings)\n",
    "            \n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"   Processed {i + len(batch_texts)}/{len(texts)} texts\")\n",
    "        \n",
    "        embeddings = np.vstack(embeddings)\n",
    "        print(f\"‚úÖ Extracted embeddings shape: {embeddings.shape}\")\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Extract text features\n",
    "text_extractor = TextFeatureExtractor()\n",
    "product_texts = text_extractor.prepare_text_data(products_df)\n",
    "\n",
    "# Extract BERT embeddings (this might take a few minutes)\n",
    "try:\n",
    "    text_embeddings = text_extractor.extract_embeddings(product_texts)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è BERT extraction failed: {e}\")\n",
    "    print(\"üîÑ Using simulated embeddings for demonstration\")\n",
    "    # Create simulated embeddings for demo\n",
    "    text_embeddings = np.random.normal(0, 1, (len(products_df), 768))\n",
    "\n",
    "print(f\"üìä Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Visualize text embeddings using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply PCA for visualization\n",
    "scaler = StandardScaler()\n",
    "text_embeddings_scaled = scaler.fit_transform(text_embeddings)\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "text_embeddings_2d = pca.fit_transform(text_embeddings_scaled)\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# Color by category\n",
    "for category in products_df['category'].unique():\n",
    "    mask = products_df['category'] == category\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=text_embeddings_2d[mask, 0],\n",
    "        y=text_embeddings_2d[mask, 1],\n",
    "        mode='markers',\n",
    "        name=category,\n",
    "        text=products_df[mask]['title'],\n",
    "        hovertemplate='<b>%{text}</b><br>Category: ' + category\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Product Text Embeddings (PCA Visualization)',\n",
    "    xaxis_title=f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)',\n",
    "    yaxis_title=f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)',\n",
    "    height=600\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"üí° PCA explains {sum(pca.explained_variance_ratio_):.1%} of variance in first 2 components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af421f1",
   "metadata": {},
   "source": [
    "## 4. Image Feature Extraction with CNN\n",
    "\n",
    "Extracting visual features from product images using pre-trained CNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0874fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-based image feature extraction\n",
    "class ImageFeatureExtractor:\n",
    "    def __init__(self, model_name='resnet50'):\n",
    "        # Load pre-trained ResNet model\n",
    "        if model_name == 'resnet50':\n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "            # Remove the final classification layer\n",
    "            self.model = nn.Sequential(*list(self.model.children())[:-1])\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Image preprocessing\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def simulate_image_features(self, n_products, feature_dim=2048):\n",
    "        \"\"\"Simulate image features based on product categories\"\"\"\n",
    "        print(f\"üñºÔ∏è  Simulating image features for {n_products} products...\")\n",
    "        \n",
    "        # Create category-based features\n",
    "        category_features = {\n",
    "            'Electronics': np.random.normal(0.5, 0.3, feature_dim),\n",
    "            'Books': np.random.normal(-0.3, 0.2, feature_dim),\n",
    "            'Home_and_Kitchen': np.random.normal(0.2, 0.4, feature_dim),\n",
    "            'Clothing': np.random.normal(-0.1, 0.3, feature_dim),\n",
    "            'Sports': np.random.normal(0.3, 0.2, feature_dim),\n",
    "            'Toys': np.random.normal(-0.2, 0.4, feature_dim),\n",
    "            'Beauty': np.random.normal(0.1, 0.3, feature_dim)\n",
    "        }\n",
    "        \n",
    "        image_features = []\n",
    "        for _, product in products_df.iterrows():\n",
    "            category = product['category']\n",
    "            base_features = category_features.get(category, np.zeros(feature_dim))\n",
    "            \n",
    "            # Add product-specific noise\n",
    "            product_features = base_features + np.random.normal(0, 0.1, feature_dim)\n",
    "            image_features.append(product_features)\n",
    "        \n",
    "        image_features = np.array(image_features)\n",
    "        print(f\"‚úÖ Generated image features shape: {image_features.shape}\")\n",
    "        \n",
    "        return image_features\n",
    "    \n",
    "    def extract_real_features(self, image_paths, batch_size=32):\n",
    "        \"\"\"Extract features from real images (if available)\"\"\"\n",
    "        # This would be used if you have actual product images\n",
    "        print(\"üîÑ Real image feature extraction not implemented - using simulated features\")\n",
    "        return self.simulate_image_features(len(image_paths))\n",
    "\n",
    "# Extract image features\n",
    "image_extractor = ImageFeatureExtractor()\n",
    "\n",
    "# Simulate image features (in real scenario, would process actual images)\n",
    "image_features = image_extractor.simulate_image_features(len(products_df))\n",
    "\n",
    "# Visualize image features using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"üîÑ Running t-SNE on image features...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "image_features_2d = tsne.fit_transform(image_features[:500])  # Sample for speed\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "for category in products_df['category'].unique():\n",
    "    mask = (products_df['category'] == category).iloc[:500]  # Match sample size\n",
    "    if mask.any():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=image_features_2d[mask, 0],\n",
    "            y=image_features_2d[mask, 1],\n",
    "            mode='markers',\n",
    "            name=category,\n",
    "            text=products_df[mask]['title'],\n",
    "            hovertemplate='<b>%{text}</b><br>Category: ' + category\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Product Image Features (t-SNE Visualization)',\n",
    "    xaxis_title='t-SNE 1',\n",
    "    yaxis_title='t-SNE 2',\n",
    "    height=600\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Combine text and image features\n",
    "print(\"üîó Combining text and image features...\")\n",
    "combined_features = np.concatenate([text_embeddings, image_features], axis=1)\n",
    "print(f\"‚úÖ Combined features shape: {combined_features.shape}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_names = ([f'text_dim_{i}' for i in range(text_embeddings.shape[1])] + \n",
    "                [f'image_dim_{i}' for i in range(image_features.shape[1])])\n",
    "\n",
    "print(f\"üìä Feature Summary:\")\n",
    "print(f\"   Text features: {text_embeddings.shape[1]} dimensions\")\n",
    "print(f\"   Image features: {image_features.shape[1]} dimensions\")\n",
    "print(f\"   Combined features: {combined_features.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f81849",
   "metadata": {},
   "source": [
    "## 5. Collaborative Filtering Implementation\n",
    "\n",
    "Building matrix factorization models for user-item interactions using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e428da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative Filtering with Matrix Factorization\n",
    "class CollaborativeFilteringModel(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # User and item embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.normal_(self.user_embedding.weight, 0, 0.1)\n",
    "        nn.init.normal_(self.item_embedding.weight, 0, 0.1)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # Get embeddings\n",
    "        user_emb = self.dropout(self.user_embedding(user_ids))\n",
    "        item_emb = self.dropout(self.item_embedding(item_ids))\n",
    "        \n",
    "        # Get biases\n",
    "        user_bias = self.user_bias(user_ids).squeeze()\n",
    "        item_bias = self.item_bias(item_ids).squeeze()\n",
    "        \n",
    "        # Compute prediction\n",
    "        interaction = (user_emb * item_emb).sum(dim=1)\n",
    "        prediction = interaction + user_bias + item_bias + self.global_bias\n",
    "        \n",
    "        return prediction, user_emb, item_emb\n",
    "\n",
    "# Dataset for collaborative filtering\n",
    "class CFDataset(Dataset):\n",
    "    def __init__(self, user_ids, item_ids, ratings):\n",
    "        self.user_ids = torch.LongTensor(user_ids)\n",
    "        self.item_ids = torch.LongTensor(item_ids)\n",
    "        self.ratings = torch.FloatTensor(ratings)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.item_ids[idx], self.ratings[idx]\n",
    "\n",
    "# Prepare collaborative filtering data\n",
    "n_users = filtered_reviews['user_idx'].nunique()\n",
    "n_items = filtered_reviews['item_idx'].nunique()\n",
    "\n",
    "print(f\"ü§ù Collaborative Filtering Setup:\")\n",
    "print(f\"   Users: {n_users}\")\n",
    "print(f\"   Items: {n_items}\")\n",
    "print(f\"   Interactions: {len(filtered_reviews)}\")\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = CFDataset(\n",
    "    train_df['user_idx'].values,\n",
    "    train_df['item_idx'].values,\n",
    "    train_df['rating'].values\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = CFDataset(\n",
    "    test_df['user_idx'].values,\n",
    "    test_df['item_idx'].values,\n",
    "    test_df['rating'].values\n",
    ") if len(test_df) > 0 else None\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) if test_dataset else None\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cf_model = CollaborativeFilteringModel(n_users, n_items, embedding_dim=64)\n",
    "cf_model.to(device)\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(cf_model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"‚úÖ CF Model initialized with {sum(p.numel() for p in cf_model.parameters())} parameters\")\n",
    "\n",
    "# Training function\n",
    "def train_cf_model(model, train_loader, optimizer, criterion, epochs=20):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    print(\"üöÄ Training Collaborative Filtering Model...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for user_ids, item_ids, ratings in train_loader:\n",
    "            user_ids, item_ids, ratings = user_ids.to(device), item_ids.to(device), ratings.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            predictions, user_emb, item_emb = model(user_ids, item_ids)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            \n",
    "            # L2 regularization\n",
    "            l2_reg = torch.norm(user_emb, p=2) + torch.norm(item_emb, p=2)\n",
    "            loss += 0.01 * l2_reg\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"   Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "# Train the model\n",
    "train_losses = train_cf_model(cf_model, train_loader, optimizer, criterion, epochs=20)\n",
    "\n",
    "# Plot training progress\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(1, len(train_losses)+1)),\n",
    "    y=train_losses,\n",
    "    mode='lines+markers',\n",
    "    name='Training Loss'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Collaborative Filtering Training Progress',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss (MSE)',\n",
    "    height=400\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"‚úÖ Training completed. Final loss: {train_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7cef22",
   "metadata": {},
   "source": [
    "## 6. Multi-Modal Fusion Architecture\n",
    "\n",
    "Designing and implementing the neural network that combines text, image, and collaborative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Modal Fusion Architecture\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim=128):\n",
    "        super().__init__()\n",
    "        self.attention_dim = attention_dim\n",
    "        self.W = nn.Linear(input_dim, attention_dim)\n",
    "        self.U = nn.Linear(attention_dim, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, input_dim)\n",
    "        u = self.tanh(self.W(x))\n",
    "        attention_weights = F.softmax(self.U(u), dim=1)\n",
    "        return attention_weights\n",
    "\n",
    "class MultiModalRecommendationModel(nn.Module):\n",
    "    def __init__(self, n_users, n_items, text_dim=768, image_dim=2048, \n",
    "                 embedding_dim=128, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Collaborative filtering components\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # Text feature projection\n",
    "        self.text_projection = nn.Sequential(\n",
    "            nn.Linear(text_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\\n        \\n        # Image feature projection\\n        self.image_projection = nn.Sequential(\\n            nn.Linear(image_dim, embedding_dim),\\n            nn.ReLU(),\\n            nn.Dropout(dropout)\\n        )\\n        \\n        # Attention mechanisms\\n        self.user_attention = AttentionLayer(embedding_dim)\\n        self.item_attention = AttentionLayer(embedding_dim)\\n        self.content_attention = AttentionLayer(embedding_dim * 2)  # text + image\\n        \\n        # Fusion network\\n        fusion_input_dim = embedding_dim * 4  # user + item + text + image\\n        \\n        fusion_layers = []\\n        prev_dim = fusion_input_dim\\n        \\n        for hidden_dim in hidden_dims:\\n            fusion_layers.extend([\\n                nn.Linear(prev_dim, hidden_dim),\\n                nn.ReLU(),\\n                nn.Dropout(dropout),\\n                nn.BatchNorm1d(hidden_dim)\\n            ])\\n            prev_dim = hidden_dim\\n        \\n        fusion_layers.append(nn.Linear(prev_dim, 1))\\n        self.fusion_network = nn.Sequential(*fusion_layers)\\n        \\n        # Content-based fallback for cold-start\\n        self.content_network = nn.Sequential(\\n            nn.Linear(embedding_dim * 2, 64),\\n            nn.ReLU(),\\n            nn.Dropout(dropout),\\n            nn.Linear(64, 1)\\n        )\\n        \\n        # Initialize embeddings\\n        nn.init.normal_(self.user_embedding.weight, 0, 0.1)\\n        nn.init.normal_(self.item_embedding.weight, 0, 0.1)\\n        nn.init.zeros_(self.user_bias.weight)\\n        nn.init.zeros_(self.item_bias.weight)\\n    \\n    def forward(self, user_ids, item_ids, text_features=None, image_features=None):\\n        batch_size = user_ids.size(0)\\n        \\n        # Collaborative filtering embeddings\\n        user_emb = self.user_embedding(user_ids)\\n        item_emb = self.item_embedding(item_ids)\\n        user_bias = self.user_bias(user_ids).squeeze()\\n        item_bias = self.item_bias(item_ids).squeeze()\\n        \\n        # Process content features\\n        if text_features is not None:\\n            text_emb = self.text_projection(text_features)\\n        else:\\n            text_emb = torch.zeros(batch_size, user_emb.size(1), device=user_emb.device)\\n        \\n        if image_features is not None:\\n            image_emb = self.image_projection(image_features)\\n        else:\\n            image_emb = torch.zeros(batch_size, user_emb.size(1), device=user_emb.device)\\n        \\n        # Apply attention to content features\\n        content_features = torch.cat([text_emb, image_emb], dim=1)\\n        content_attention_weights = self.content_attention(content_features)\\n        attended_content = content_attention_weights * content_features\\n        \\n        # Split back to text and image\\n        mid_point = attended_content.size(1) // 2\\n        attended_text = attended_content[:, :mid_point]\\n        attended_image = attended_content[:, mid_point:]\\n        \\n        # Fusion\\n        fusion_input = torch.cat([user_emb, item_emb, attended_text, attended_image], dim=1)\\n        main_prediction = self.fusion_network(fusion_input).squeeze()\\n        \\n        # Content-based prediction for cold-start\\n        content_input = torch.cat([text_emb, image_emb], dim=1)\\n        content_prediction = self.content_network(content_input).squeeze()\\n        \\n        # Final prediction combines collaborative filtering bias with neural network\\n        final_prediction = main_prediction + user_bias + item_bias + self.global_bias\\n        \\n        return {\\n            'prediction': final_prediction,\\n            'content_prediction': content_prediction,\\n            'user_embedding': user_emb,\\n            'item_embedding': item_emb,\\n            'text_embedding': text_emb,\\n            'image_embedding': image_emb\\n        }\\n\\n# Prepare multi-modal dataset\\nclass MultiModalDataset(Dataset):\\n    def __init__(self, user_ids, item_ids, ratings, text_features, image_features):\\n        self.user_ids = torch.LongTensor(user_ids)\\n        self.item_ids = torch.LongTensor(item_ids)\\n        self.ratings = torch.FloatTensor(ratings)\\n        self.text_features = torch.FloatTensor(text_features)\\n        self.image_features = torch.FloatTensor(image_features)\\n        \\n    def __len__(self):\\n        return len(self.user_ids)\\n    \\n    def __getitem__(self, idx):\\n        return (\\n            self.user_ids[idx], \\n            self.item_ids[idx], \\n            self.ratings[idx],\\n            self.text_features[self.item_ids[idx]],\\n            self.image_features[self.item_ids[idx]]\\n        )\\n\\n# Create item feature lookup\\nvalid_items = preprocessor.item_encoder.classes_\\nitem_text_features = text_embeddings\\nitem_image_features = image_features\\n\\n# Create multi-modal datasets\\nmm_train_dataset = MultiModalDataset(\\n    train_df['user_idx'].values,\\n    train_df['item_idx'].values,\\n    train_df['rating'].values,\\n    item_text_features,\\n    item_image_features\\n)\\n\\nmm_test_dataset = MultiModalDataset(\\n    test_df['user_idx'].values,\\n    test_df['item_idx'].values,\\n    test_df['rating'].values,\\n    item_text_features,\\n    item_image_features\\n) if len(test_df) > 0 else None\\n\\n# Data loaders\\nmm_train_loader = DataLoader(mm_train_dataset, batch_size=256, shuffle=True)\\nmm_test_loader = DataLoader(mm_test_dataset, batch_size=256, shuffle=False) if mm_test_dataset else None\\n\\n# Initialize multi-modal model\\nmm_model = MultiModalRecommendationModel(\\n    n_users=n_users,\\n    n_items=n_items,\\n    text_dim=text_embeddings.shape[1],\\n    image_dim=image_features.shape[1],\\n    embedding_dim=128,\\n    hidden_dims=[256, 128, 64],\\n    dropout=0.3\\n)\\nmm_model.to(device)\\n\\nprint(f\\\"üöÄ Multi-Modal Model initialized:\\\")\\nprint(f\\\"   Parameters: {sum(p.numel() for p in mm_model.parameters()):,}\\\")\\nprint(f\\\"   Text features: {text_embeddings.shape[1]} dims\\\")\\nprint(f\\\"   Image features: {image_features.shape[1]} dims\\\")\\nprint(f\\\"   Total content features: {text_embeddings.shape[1] + image_features.shape[1]} dims\\\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
